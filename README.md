<h1>Problem Statement 1</h1>

<h2>Rice Production Analysis and Prediction</h2>

<h3>Model Used:</h3>

We employed the Autoregressive Integrated Moving Average (ARIMA) model to forecast rice production. Specifically, we utilized the AutoARIMA method available in the pmdarima (Pyramid ARIMA) library in Python for time series forecasting.

<h3>Methodology:</h3>

<H4>Data Exploration and Processing</H4>

We began our analysis by performing all necessary Exploratory Data Analysis (EDA) steps to gain insights into the dataset's structure and characteristics.

<h4>Data Visualisation</h4>

To gain deeper insights into the rice production data, we employed various visualization techniques, including:

<h4>Box Plots</h4> Used to visualize the distribution of rice production across different states and identify outliers.
<h4>Pie Charts</h4> Provided a visual representation of the contribution of each state or union territory to the total rice production.
<h4>Heatmaps</h4> Illustrated the correlation between rice production and other relevant factors, such as rainfall, temperature, and agricultural practices.

<h1>Problem Statement 2</h1>

<h2>Multimodal Hate Speech Detection Project</h2>

<h3>Models Used:</h3>

We have implemented several models and evaluated their performance based on the Area Under the Curve (AUC) metric:

<h4>1. Visual BERT</h4>

Description: This model utilizes a Visual BERT architecture, which combines the power of BERT (Bidirectional Encoder Representations from Transformers) with visual information extracted from images. It achieves a competitive AUC score of 0.71.

<H4>2. Image Captioning followed by Classification using BERT</H4>

Description: In this approach, we first generate captions for images using ResNet and then classify the generated captions using a BERT-based classifier. Our approach achieved an AUC score of 0.47.

<h4>3. Contrastive Language-Image Pre-training (CLIP):</h4>

Description: CLIP is a state-of-the-art multimodal model that learns representations by contrasting images and text. Our implementation achieved an AUC score of 0.60.

<h5>4. FastText and VGG19 Embeddings:</h5>

Description: This model combines FastText word embeddings with image embeddings generated by the VGG19 convolutional neural network. It achieves an AUC score of 0.54.
