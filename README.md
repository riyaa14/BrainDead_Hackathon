[REPORT LINK](https://www.canva.com/design/DAGBi0HYSrE/_hTD0LN3rP47EzJ-18ctQA/view?utm_content=DAGBi0HYSrE&utm_campaign=designshare&utm_medium=link&utm_source=editor)

<h1>Problem Statement 1</h1>
GitHub Repository - https://github.com/AngRoy/BrainDead2k24_Problem1

<h2>Rice Production Analysis and Prediction</h2>

⭐ <h3>Methodology:</h3>

<h4>Data Exploration and Processing</h4>
We began our analysis by performing all necessary Exploratory Data Analysis (EDA) steps to gain insights into the dataset's structure and characteristics.

<h4>Data Visualisation</h4>
To gain deeper insights into the rice production data, we employed various visualization techniques, including:

<h4>Box Plots</h4> Used to visualize the distribution of rice production across different states and identify outliers.
<h4>Pie Charts</h4> Provided a visual representation of the contribution of each state or union territory to the total rice production.
<h4>Heatmaps</h4> Illustrated the correlation between rice production and other relevant factors, such as rainfall, temperature, and agricultural practices.

⭐ <h3>Model Used for Forecasting:</h3>
We employed the Autoregressive Integrated Moving Average (ARIMA) model to forecast rice production. Specifically, we utilized the AutoARIMA method available in the pmdarima (Pyramid ARIMA) library in Python for time series forecasting.

<h1>Problem Statement 2</h1>
GitHub Repository - https://github.com/riyaa14/BrainDead_Hackathon

<h2>Multimodal Hate Speech Detection Project</h2>

⭐ <h3>Models Used:</h3>
We have implemented several models and evaluated their performance based on the Area Under the Curve (AUC) metric:

<h4>1. Visual BERT</h4>

Description: This model utilizes a Visual BERT architecture, which combines the power of BERT (Bidirectional Encoder Representations from Transformers) with visual information extracted from images. It achieves a competitive AUC score of 0.71.

[VisualBERT Code](https://github.com/riyaa14/BrainDead_Hackathon/blob/main/VisualBERT.ipynb)

<H4>2. Image Captioning followed by Classification using BERT</H4>

Description: In this approach, we first generate captions for images using ResNet and then classify the generated captions using a BERT-based classifier. Our approach achieved an AUC score of 0.47.

[Image Captions Generation Code](https://github.com/riyaa14/BrainDead_Hackathon/blob/main/image_captioning.ipynb)

[Classification using BERT](https://github.com/riyaa14/BrainDead_Hackathon/blob/main/BERT_Model.ipynb)

<h4>3. Contrastive Language-Image Pre-training (CLIP)</h4>

Description: CLIP is a state-of-the-art multimodal model that learns representations by contrasting images and text. Our implementation achieved an AUC score of 0.60.

[Clip Model](https://github.com/riyaa14/BrainDead_Hackathon/blob/main/CLIP_Model.ipynb)

<h4>4. FastText and VGG19 Embeddings</h4>

Description: This model combines FastText word embeddings with image embeddings generated by the VGG19 convolutional neural network. It achieves an AUC score of 0.54.

[Image Embeddings Generation using vgg19](https://github.com/riyaa14/BrainDead_Hackathon/blob/main/vgg19-Image-embeddings-generation.ipynb)

[Text Embeddings Generation using Fasttext](https://github.com/riyaa14/BrainDead_Hackathon/fasttext-text-embeddings-generation.ipynb)

[Final Model](https://github.com/riyaa14/BrainDead_Hackathon/blob/main/FastText%2BVGG19_Embeddings.ipynb) which combines these embeddings and classifies the memes 

⭐ <h3> Future Optimization Strategies - </h3>

1. Ensembling these models: Since every model has its strengths and weaknesses, by combining the predictions of multiple models, ensembling can mitigate individual model biases and errors, leading to improved overall performance.
2. We can improve on Image Caption Generation and relating it with the text data.
3. Better Image embeddings and text embeddings can further improve results.
4. Model Interpretability: We can enhance the interpretability of our models by incorporating attention mechanisms or visualization techniques.
5. Model Architecture: Experimenting with different model architectures. This could involve modifying network depths, adding attention mechanisms, or incorporating novel components specific to multimodal learning. 
